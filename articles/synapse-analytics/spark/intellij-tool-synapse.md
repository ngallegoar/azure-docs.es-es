---
title: 'Tutorial: Azure Toolkit for IntelliJ (aplicación Spark)'
description: 'Tutorial: Uso de Azure Toolkit for IntelliJ para desarrollar aplicaciones Spark escritas en Scala y enviarlas a un grupo de Apache Spark (versión preliminar)'
services: synapse-analytics
author: v-jiche
ms.author: v-jiche
ms.reviewer: jrasnick, carlrab
ms.service: synapse-analytics
ms.topic: tutorial
ms.date: 04/15/2020
ms.openlocfilehash: b344ae50d921c33a5e8ddd344e08ec86179668e9
ms.sourcegitcommit: 1de57529ab349341447d77a0717f6ced5335074e
ms.translationtype: HT
ms.contentlocale: es-ES
ms.lasthandoff: 06/09/2020
ms.locfileid: "84608764"
---
# <a name="tutorial-create-apache-spark-applications-with-intellij-using-synapse-synapse-analytics-workspaces-preview"></a>Tutorial: Creación de aplicaciones Apache Spark con IntelliJ mediante Synapse Analytics (vista previa de áreas de trabajo)

Este tutorial demuestra cómo usar el complemento de Azure Toolkit for IntelliJ para desarrollar aplicaciones Apache Spark escritas en [Scala](https://www.scala-lang.org/) y enviarlas a continuación a un grupo de Spark (versión preliminar) directamente desde el entorno de desarrollo integrado (IDE) de IntelliJ. Puede usar el complemento de varias maneras:

- Desarrollar y enviar una aplicación Spark en Scala a un grupo de Spark.
- Acceder a los recursos de los grupos de Spark.
- Desarrollar y ejecutar localmente una aplicación Spark en Scala.

En este tutorial, aprenderá a:
> [!div class="checklist"]
>
> - Usar el complemento Azure Toolkit for IntelliJ
> - Desarrollar aplicaciones de Apache Spark
> - Envío de una aplicación a grupos de Spark

## <a name="prerequisites"></a>Requisitos previos

- [IntelliJ IDEA Community Version](https://www.jetbrains.com/idea/download/download-thanks.html?platform=windows&code=IIC).
- Complemento de Azure Toolkit 3.27.0-2019.2: instalación desde el [Repositorio de complementos de IntelliJ](/java/azure/intellij/azure-toolkit-for-intellij-installation?toc=/azure/synapse-analytics/toc.json&bc=/azure/synapse-analytics/breadcrumb/toc.json)
- [JDK (Versión 1.8)](https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html).
- Complemento de Scala: instalación desde el [Repositorio de complementos de IntelliJ](/azure/hdinsight/spark/apache-spark-intellij-tool-plugin#install-scala-plugin-for-intellij-idea).
- Este requisito previo es solo para los usuarios de Windows.

  Mientras se ejecuta la aplicación Spark en Scala local en un equipo Windows, puede producirse una excepción, como se explica en [SPARK-2356](https://issues.apache.org/jira/browse/SPARK-2356). Esta excepción se produce porque falta WinUtils.exe en Windows.
  Para solucionar este error, descargue el [archivo ejecutable WinUtils](http://public-repo-1.hortonworks.com/hdp-win-alpha/winutils.exe) y guárdelo en una ubicación como **C:\WinUtils\bin**. Después, agregue una variable de entorno **HADOOP_HOME** y establezca el valor de la variable en **C:\WinUtils**.

## <a name="create-a-spark-scala-application-for-a-spark-pool"></a>Creación de una aplicación Spark en Scala para un grupo de Spark

1. Inicie IntelliJ IDEA y seleccione **Create New Project** (Crear proyecto) para abrir la ventana **New Project** (Nuevo proyecto).
2. Seleccione **Azure Spark/HDInsight** en el panel izquierdo.
3. Seleccione **Spark Project with Samples (Scala)** (Proyecto de Spark con ejemplos [Scala]) en la ventana principal.
4. En la lista desplegable **Build tool** (Herramienta de compilación), seleccione uno de los siguientes tipos:

   - **Maven**: para agregar compatibilidad con el asistente para la creación de proyectos de Scala.
   - **SBT** para administrar las dependencias y compilar el proyecto de Scala.

    ![IDEA de IntelliJ: cuadro de diálogo New Project (Nuevo proyecto)](./media/intellij-tool-synapse/create-synapse-application01.png)

5. Seleccione **Next** (Siguiente).
6. En la ventana **New Project** (Nuevo proyecto), proporcione la siguiente información:

    | Propiedad | Descripción |
    | ----- | ----- |
    |Nombre de proyecto| Escriba un nombre. En este tutorial se usa `myApp`.|
    |Project&nbsp;location (Ubicación del proyecto)| Escriba la ubicación deseada para guardar el proyecto.|
    |Project SDK (SDK del proyecto)| Podría quedarse en blanco la primera vez que se usa IDEA. Seleccione **New...** (Nuevo...) y vaya a su JDK.|
    |Versión de Spark|El asistente de creación integra la versión adecuada de los SDK de Spark y Scala. Synapse solo admite **Spark 2.4.0**.|
    |||

    ![Selección del SDK de Apache Spark](./media/intellij-tool-synapse/create-synapse-application02.png)

7. Seleccione **Finalizar**. El proyecto puede tardar unos minutos en estar disponible.
8. El proyecto de Spark crea automáticamente un artefacto. Para ver el artefacto, haga lo siguiente:

   a. En la barra de menús, vaya a **Archivo** > **Estructura del proyecto...** .

   b. En la ventana **Estructura del proyecto**, seleccione **Artefactos**.

   c. Seleccione **Cancelar** después de ver el artefacto.

    ![Información del artefacto en el cuadro de diálogo](./media/intellij-tool-synapse/default-artifact-dialog.png)

9. Busque **LogQuery** en **myApp** > **src** > **main** > **scala**> **sample**> **LogQuery**. En este tutorial se usa **LogQuery** para la ejecución.

   ![Comandos para crear una clase Scala desde Proyecto](./media/intellij-tool-synapse/select-run-file.png)

## <a name="connect-to-your-spark-pools"></a>Conexión a los grupos de Spark

Inicie sesión en la suscripción de Azure para conectarse a los grupos de Spark.

### <a name="sign-in-to-your-azure-subscription"></a>Inicie sesión en la suscripción de Azure

1. En la barra de menús, vaya a **Ver** > **Ventanas de herramientas** > **Azure Explorer**.

   ![IntelliJ IDEA muestra Azure Explorer](./media/intellij-tool-synapse/show-azure-explorer1.png)

2. En Azure Explorer, haga clic con el botón derecho en el nodo **Azure** y después seleccione **Iniciar sesión**.

   ![IntelliJ IDEA: clic con el botón derecho en Azure Explorer](./media/intellij-tool-synapse/explorer-rightclick-azure.png)

3. En el cuadro de diálogo de **inicio de sesión en Azure**, seleccione **Inicio de sesión del dispositivo** y, a continuación, **Iniciar sesión**.

    ![IntelliJ IDEA: Inicio de sesión en Azure](./media/intellij-tool-synapse/intellij-view-explorer2.png)

4. En el cuadro de diálogo **Inicio de sesión del dispositivo de Azure**, haga clic en **Copiar y abrir**.

   ![IntelliJ IDEA: inicio de sesión del dispositivo de Azure](./media/intellij-tool-synapse/intellij-view-explorer5.png)

5. En la interfaz del explorador, pegue el código y, a continuación, haga clic en **Siguiente**.

   ![Cuadro de diálogo del código de entradas de Microsoft para HDI](./media/intellij-tool-synapse/intellij-view-explorer6.png)

6. Escriba sus credenciales de Azure y, a continuación, cierre el explorador.

   ![Cuadro de diálogo del correo electrónico de entradas de Microsoft para HDI](./media/intellij-tool-synapse/intellij-view-explorer7.png)

7. Cuando haya iniciado sesión, en el cuadro de diálogo **Select Subscriptions** (Seleccionar suscripciones) se enumeran todas las suscripciones de Azure asociadas a las credenciales. Seleccione la suscripción y, a continuación, haga clic en **Seleccionar**.

    ![Cuadro de diálogo Seleccionar suscripciones](./media/intellij-tool-synapse/Select-Subscriptions.png)

8. En **Azure Explorer**, expanda **Apache Spark en Synapse** para ver las áreas de trabajo de las suscripciones.

    ![IDEA de IntelliJ: vista principal de Azure Explorer](./media/intellij-tool-synapse/azure-explorer-workspace.png)

9. Para ver los grupos de Spark, puede expandir un área de trabajo.

    ![Azure Explorer: cuentas de almacenamiento](./media/intellij-tool-synapse/azure-explorer-pool.png)

## <a name="remote-run-a-spark-scala-application-on-a-spark-pool"></a>Ejecución remota de una aplicación Spark en Scala en un grupo de Spark

Después de crear una aplicación en Scala, puede ejecutarla de forma remota.

1. Haga clic en el icono para abrir la ventana **Configuraciones de ejecución/depuración**.

    ![Envío de aplicación Spark a comando HDInsight](./media/intellij-tool-synapse/open-configuration-window.png)

2. En el ventana de diálogo **Configuraciones de ejecución/depuración**, haga clic en **+** y, a continuación, seleccione **Apache Spark en Synapse**.

    ![Envío de aplicación Spark a comando HDInsight](./media/intellij-tool-synapse/create-synapse-configuration02.png)

3. En la ventana **Configuraciones de ejecución/depuración**, proporcione los valores siguientes y, a continuación, seleccione **Aceptar**:

    |Propiedad |Value |
    |----|----|
    |Grupos de Spark|Seleccione los grupos de Spark en los que quiere ejecutar la aplicación.|
    |Seleccione un artefacto para enviarlo|Deje la configuración predeterminada.|
    |Nombre de clase principal|el valor predeterminado es la clase principal del archivo seleccionado. Puede cambiar la clase si selecciona los puntos suspensivos ( **...** ) y elige otra clase.|
    |Configuraciones del trabajo|Puede cambiar las claves y valores predeterminados. Para más información, consulte [API de REST de Apache Livy](http://livy.incubator.apache.org./docs/latest/rest-api.html).|
    |Argumentos de la línea de comandos|Puede especificar los argumentos divididos por un espacio para la clase principal, si es necesario.|
    |Archivos jar a los que se hace referencia y archivos a los que se hace referencia|puede escribir las rutas de acceso de los archivos y los Jar a los que se hace referencia, si los hubiera. También puede examinar archivos en el sistema de archivos virtual de Azure, que actualmente solo admite el clúster de ADLS Gen2. Para obtener más información: [Configuración de Apache Spark](https://spark.apache.org/docs/latest/configuration.html#runtime-environment) y [Cómo cargar recursos en el clúster](../../storage/blobs/storage-quickstart-blobs-storage-explorer.md?toc=/azure/synapse-analytics/toc.json&bc=/azure/synapse-analytics/breadcrumb/toc.json).|
    |Almacenamiento de carga del trabajo|Expanda para mostrar opciones adicionales.|
    |Tipo de almacenamiento|Seleccione la opción para **usar Azure Blob para cargar** en la lista desplegable.|
    |Cuenta de almacenamiento|Escriba su cuenta de Storage.|
    |Clave de almacenamiento|Escriba su clave de almacenamiento.|
    |Contenedor de almacenamiento|Seleccione su contenedor de almacenamiento en la lista desplegable una vez que se hayan escrito **Cuenta de Storage** y **Clave de almacenamiento**.|

    ![Cuadro de diálogo de envío de Spark](./media/intellij-tool-synapse/create-synapse-configuration03.png)

4. Haga clic en el icono **SparkJobRun** para enviar el proyecto al grupo de Spark seleccionado. La pestaña **Remote Spark Job in Cluster** (Trabajo de Spark remoto en clúster) muestra el progreso de la ejecución del trabajo en la parte inferior. Puede detener la aplicación si hace clic en el botón rojo.

    ![Ventana de envío de Apache Spark](./media/intellij-tool-synapse/remotely-run-synapse.png)

    ![Cuadro de diálogo de envío de Spark](./media/intellij-tool-synapse/remotely-run-result.png)

## <a name="local-rundebug-apache-spark-applications"></a>Ejecución/depuración local de aplicaciones Apache Spark

Puede seguir las instrucciones siguientes para configurar la ejecución local y la depuración local para el trabajo de Apache Spark.

### <a name="scenario-1-do-local-run"></a>Escenario 1: Ejecución local

1. Abra el cuadro de diálogo **Configuraciones de ejecución/depuración** y seleccione el signo más ( **+** ). A continuación, seleccione la opción **Apache Spark en Synapse**. Escriba la información de **Nombre** y **Nombre de clase principal** para guardar.

    ![Ejecución local de las configuraciones de ejecución y depuración de IntelliJ](./media/intellij-tool-synapse/local-run-synapse.png)

    - Las variables de entorno y la ubicación de WinUtils.exe son solo para los usuarios de Windows.
    - Variables de entorno: la variable de entorno del sistema se puede detectar automáticamente si se ha establecido antes y no es necesario agregarla manualmente.
    - [Ubicación de WinUtils.exe](http://public-repo-1.hortonworks.com/hdp-win-alpha/winutils.exe): para especificar la ubicación de WinUtils, haga clic en el icono de carpeta de la derecha.

2. A continuación, haga clic en el botón de reproducción local.

    ![Ejecución local de las configuraciones de ejecución y depuración de IntelliJ](./media/intellij-tool-synapse/local-run-synapse01.png)

3. Una vez completada la ejecución local, si el script incluye una salida, puede comprobar el archivo de salida desde **data** > **__default__**.

    ![Resultado de la ejecución local del proyecto de IntelliJ](./media/intellij-tool-synapse/spark-local-run-result.png)

### <a name="scenario-2-do-local-debugging"></a>Escenario 2: Depuración local

1. Abra el script **LogQuery** y establezca puntos de interrupción.
2. Haga clic en el icono **Depuración local** para realizar la depuración local.

    ![Resultado de la ejecución local del proyecto de IntelliJ](./media/intellij-tool-synapse/local-debug-synapse.png)

## <a name="access-and-manage-synapse-workspace"></a>Acceso y administración del área de trabajo de Synapse

Puede realizar diferentes operaciones en Azure Explorer dentro de Azure Toolkit for IntelliJ. En la barra de menús, vaya a **Ver** > **Ventanas de herramientas** > **Azure Explorer**.

### <a name="launch-workspace"></a>Inicio del área de trabajo

1. En Azure Explorer, vaya a **Apache Spark en Synapse** y, a continuación, expándalo.

    ![IDEA de IntelliJ: vista principal de Azure Explorer](./media/intellij-tool-synapse/azure-explorer-workspace.png)

2. Haga clic con el botón derecho en un área de trabajo y, a continuación, seleccione **Iniciar área de trabajo**, se abrirá el sitio web.

    ![Detalles de aplicación en vista de trabajos de Spark](./media/intellij-tool-synapse/launch-workspace-synapse.png)

    ![Detalles de aplicación en vista de trabajos de Spark](./media/intellij-tool-synapse/launch-workspace-result.png)

## <a name="spark-console"></a>Consola de Spark

Puede ejecutar la consola local de Spark (Scala) o la consola de sesión interactiva de Spark Livy (Scala).

### <a name="spark-local-console-scala"></a>Consola local de Spark (Scala)

Asegúrese de que cumple el requisito previo de WINUTILS.EXE.

1. En la barra de menús, vaya a **Ejecutar** > **Editar configuraciones...**
2. En la ventana **Configuraciones de ejecución/depuración**, en el panel izquierdo, vaya a **Apache Spark en Synapse** >  **[Spark en Synapse] myApp**.
3. En la ventana principal, seleccione la pestaña **Ejecutar de forma local**.
4. Proporcione los valores siguientes y seleccione **Aceptar**:

    |Propiedad |Value |
    |----|----|
    |Variables de entorno|Asegúrese de que el valor de HADOOP_HOME sea correcto.|
    |Ubicación de WINUTILS.exe|Asegúrese de que la ruta de acceso sea correcta.|

    ![Configuración de conjunto de consola local](./media/intellij-tool-synapse/local-console-synapse01.png)

5. En el proyecto, vaya a **myApp** > **src** > **main** > **scala** > **myApp**.
6. En la barra de menús, vaya a **Herramientas** > **Consola de Spark** > **Ejecutar consola local de Spark (Scala)** .
7. Pueden aparecer dos cuadros de diálogo para preguntarle si quiere corregir automáticamente las dependencias. Si es así, seleccione **Autocorrección**.

    ![IDEA de IntelliJ: cuadro de diálogo 1 de corrección automática de Spark](./media/intellij-tool-synapse/intellij-console-autofix1.png)

    ![IDEA de IntelliJ: cuadro de diálogo 2 de corrección automática de Spark](./media/intellij-tool-synapse/intellij-console-autofix2.png)

8. El aspecto de la consola debería ser similar al de la siguiente imagen. En la ventana de la consola, escriba `sc.appName` y presione CTRL + Entrar. Se muestra el resultado. Para detener la consola local, haga clic en el botón rojo.

    ![IDEA de IntelliJ: resultado de la consola local](./media/intellij-tool-synapse/local-console-result.png)

### <a name="spark-livy-interactive-session-console-scala"></a>Consola de sesión interactiva de Spark Livy (Scala)

Solo se admite en IntelliJ 2018.2 y 2018.3.

1. En la barra de menús, vaya a **Ejecutar** > **Editar configuraciones...**

2. En la ventana **Configuraciones de ejecución/depuración**, en el panel izquierdo, vaya a **Apache Spark en Synapse** >  **[Spark en Synapse] myApp**.

3. En la ventana principal, seleccione la pestaña **Remotely Run in Cluster** (Ejecutar en clúster de forma remota).

4. Proporcione los valores siguientes y seleccione **Aceptar**:

    |Propiedad |Value |
    |----|----|
    |Grupos de Spark|Seleccione los grupos de Spark en los que quiere ejecutar la aplicación.|
    ||

    ![Configuración de conjunto de consola interactiva](./media/intellij-tool-synapse/interactive-console-configuration.png)

5. En el proyecto, vaya a **myApp** > **src** > **main** > **scala** > **myApp**.

6. En la barra de menús, vaya a **Herramientas** > **Consola de Spark** > **Ejecutar consola de sesión interactiva de Spark Livy (Scala)** .
7. El aspecto de la consola debería ser similar al de la siguiente imagen. En la ventana de la consola, escriba `sc.appName` y presione CTRL + Entrar. Se muestra el resultado. Para detener la consola local, haga clic en el botón rojo.

    ![IDEA de IntelliJ: resultado de la consola interactiva](./media/intellij-tool-synapse/interactive-console-result.png)

### <a name="send-selection-to-spark-console"></a>Envío de la selección a la consola de Spark

Resulta cómodo predecir el resultado del script mediante el envío de código a la consola local o a la consola de sesión interactiva de Livy(Scala). Puede resaltar parte del código en el archivo de Scala y hacer clic con el botón derecho en **Enviar selección a consola de Spark**. El código seleccionado se envía a la consola y se ejecuta. El resultado se muestra después del código en la consola. La consola comprueba si hay errores.

   ![Envío de la selección a la consola de Spark](./media/intellij-tool-synapse/send-selection-to-console.png)

## <a name="next-steps"></a>Pasos siguientes

- [Azure Synapse Analytics](../overview-what-is.md)
- [Creación de un grupo de Apache Spark para un área de trabajo de Azure Synapse Analytics](../../synapse-analytics/quickstart-create-apache-spark-pool-studio.md)
